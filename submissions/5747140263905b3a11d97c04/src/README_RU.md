## Анализ

Анализ words и no-words показал:

1. Большинство слов с апострофом заканчиваются на "'s". Поэтому можно сразу отсечь часть не-слов, у которых после апострофа может быть другая комбинация букв. И далее рассматривать слово до апострофа.
2. Слов с двойным и более апострофом почти нет в словах.
3. Слова длинной менее 2 букв - в большинстве нормальные слова.
4. Слова состоящие только из менее 2 разных букв разной длинны - в большинстве нормальные слова. 
5. Большинство слов состоит до 15 букв. Таким образом можно отсечь большинство не-слов.
6. Большинство похожих слов различаются только окончанием. Окончания - "ness", "ing", "ly", "s". Можно ими пренебречь при проверке.
7. Большинство слов имеют такие характеристики:
	* Не содержат более 2 одинаковых букв подряд
	* Не содержат более 4 гласных букв подряд
	* Не содержат более 5 согласных букв подряд

Так же была другая статистика, но она мало влияла на результат.


## Данные

После многочисленных попыток и проверок разных алгоритмов стало понятно, что нужны какие то данные.
Было перепробовано много алгоритмов сжатия словаря, что в принципе невозможно сжать все слова.
Максимум удалось сжать словарь с точностью 93-95% до 300-400KB с помощью префиксного бинарного дерева. 
Bloom фильтр тоже не давал хороших результатов. Но он навел на одну интересную мысль.
У нас есть 65536 Bytes или 524288 bits. Это наши фактические ограничения.
Что если мы вычислим некий hash и результирующее число будем использовать как индекс для установки bit в этом массиве bits.
Получается, что у нас есть 1 bit на слово. Конечно hash могут совпадать. Но тем не менее какую то часть слов такая таблица может отсеять.
Таким образом данные создаются довольно просто - пробегаемся по всем словам и ставим метку слова в данном массиве bits. 
Сжатие gzip сжимает массив так, что остается примерно килобайт для кода. 
Данный метод позволил сильно повысить эффективность алгоритма.
Количество слов входящих в данные можно уменьшить фильтруя каждое слово по ограничениям описанным выше.
Очевидно, что чем меньше слов - тем выше вероятность правильного ответа.
После подбора всех параметров размер данных был уменьшен до 65433, что бы вместе с кодом уложиться в 65536.
Это почти никак не повлияло на тесты.

## Алгоритм

Алгоритм нацелен на отбраковку не-слов, с таким расчетом, что бы все нормальные слова проходили проверку с максимальной точностью.

1. Проверяется апостроф, если после него не 's' - возращаем false.
2. Удаляем апостроф и все после него, если он есть.
3. Если длина слова менее или равна 2 то это слово скорее всего нормальное - возращаем true.
4. Если длина слова более 15 то это слово скорее не нормальное - возращаем false.
5. Если слово имеет менее 2 разных букв, то считаем что это нормальное слово и возвращаем true. Используется только для построения таблицы.
6. Слово считается нормальным, если оно не содержит:
	* более 2 одинаковых букв подряд
	* более 4 гласных букв подряд
	* более 5 согласных букв подряд
7. Далее отсекаем окончание если оно "ness", "ing", "ly", "s". Подобрано экспериментально.
8. Так же экспериментально было подобрано что если отрезать слово по 8 буквам, то это дает дополнительный прирост.
9. Вычисляем hash и проверяем по таблице. Если bit есть то значит наше слово нормальное - возвращаем true.

## Результат

На моих тестах выходило 79,7%
На тестовой выборке в 1000 блоков выходило 79,2%
